{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "update_master_csv.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ubJoyHdDolg5"
      },
      "source": [
        "# UPDATE MASTER CSV\n",
        "\n",
        "Follow instructions to save all metadata in a single master CSV."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fzxoO89ypFZD"
      },
      "source": [
        "## Step 1\n",
        "\n",
        "Run the cell below.\n",
        "\n",
        "You will be prompted to follow a link to authorize access to the Drive. Click the link and make sure that you sign in as ptrstoryteam@gmail.com. Copy the authorization token, return to this page, paste the token into the provided box, and hit enter on your keyboard. This completes the authorization."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OgfTIZiRpKa9",
        "outputId": "f35c8ca7-2c61-4a97-a3f9-5b92e2f7d479",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# LIBRARIES\n",
        "\n",
        "import os\n",
        "import shutil\n",
        "import glob\n",
        "import csv\n",
        "from datetime import datetime\n",
        "\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "\n",
        "# MOUNTING THE DRIVE/AUTHORIZATION\n",
        "\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D49X2dBvp1UQ"
      },
      "source": [
        "## Step 2\n",
        "\n",
        "Run the cell below to complete initial setup."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tb9JE-miqmd0"
      },
      "source": [
        "# DIRECTORIES\n",
        "\n",
        "# Path to where all metadata will be stored and processed in GDrive\n",
        "FULL_PATH_CSV = \"/content/drive/My Drive/Protect the Results Story Team/10 PTR DISTRIBUTION/Footage Library/METADATA/\"\n",
        "\n",
        "# Folder for incoming (unprocessed) csvs\n",
        "INCOMING_CSV_FOLDER = \"incoming_csv/\"\n",
        "\n",
        "# Folder for processed csvs\n",
        "PROCESSED_CSV_FOLDER = \"processed_csv/\"\n",
        "\n",
        "# Folder for logs of processed csvs\n",
        "PROCESSED_LOGS_FOLDER= \"processed_logs/\"\n",
        "\n",
        "# FILES\n",
        "\n",
        "# Master CSV file \n",
        "NAME_MASTER_CSV = \"master_csv_test.csv\"\n",
        "\n",
        "# OTHER CONSTANTS\n",
        "\n",
        "# Columns of interest \n",
        "COLUMNS = [\"hash_id\", \"project_id\", \"contributor_email\", \"contributor_name\",\"time_uploaded\",\n",
        "           \"shot_title\", \"caption\", \"media_type\", \"latitude\", \"longitude\", \n",
        "           \"video_duration\", \"tags\"]\n",
        "\n",
        "# FUNCTIONS\n",
        "\n",
        "def processed_csv(csv_full_path, already_processed_hash_id):\n",
        "  df_raw = pd.read_csv(csv_full_path)\n",
        "\n",
        "  # Record the hashid that has potentially changed\n",
        "  potentially_changed_hashid = []\n",
        "\n",
        "  # Record hashid that is not processed\n",
        "  error_hashid = []\n",
        "\n",
        "  # Iterate through each row\n",
        "  for i, row in df_raw.iterrows():\n",
        "    hashid = row[\"hash_id\"]\n",
        "    if row[\"hash_id\"] in already_processed_hash_id:\n",
        "      potentially_changed_hashid.append(hashid)\n",
        "      continue\n",
        "    try: \n",
        "      # Assuming the tags are always separated by newlines\n",
        "      row[\"tags\"] = row[\"tags\"].replace(\"\\n\", \" \") # Ensures that multiple occurences of \\n are replaced by whitespaces\n",
        "      row[\"tags\"] = row[\"tags\"].split() # Split into seperate tags\n",
        "\n",
        "      # Obtain subset to insert into master CSV\n",
        "      subset_csv = row[COLUMNS ].to_frame().T\n",
        "\n",
        "      # Insert into CSV\n",
        "      subset_csv.to_csv(FULL_PATH_CSV+NAME_MASTER_CSV, header=None, mode='a')\n",
        "\n",
        "    except Exception as e:\n",
        "          print(\"\\n\")\n",
        "          print(str(e))\n",
        "          print(\"Unable to process: \" + hashid)\n",
        "          error_hashid.append((hashid, str(e)))\n",
        "          continue\n",
        "  return potentially_changed_hashid, error_hashid"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vMcZcSVCrSK-"
      },
      "source": [
        "## Step 3+\n",
        "\n",
        "Run the cell below to update the master CSV with data from incoming CSVs. Re-run the cell every time new CSVs are uploaded."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV9InVSLriJG",
        "outputId": "fc2b6d19-ed7d-4153-bf1a-3ebf9295f775",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "if(os.path.exists(FULL_PATH_CSV+NAME_MASTER_CSV)):\n",
        "  print(\"MASTER CSV ALREADY EXISTS\")\n",
        "else:\n",
        "  # Create a csv file with the columns if it does not exist\n",
        "  print(\"NO MASTER CSV FOUND, CREATING NEW FILE\")\n",
        "  pd.DataFrame(columns=COLUMNS).to_csv(FULL_PATH_CSV+NAME_MASTER_CSV)\n",
        "\n",
        "# Keep track of hashids that have been processed\n",
        "already_processed_hash_id = pd.read_csv(FULL_PATH_CSV+NAME_MASTER_CSV)[\"hash_id\"].to_list()\n",
        "\n",
        "# All files ending with .csv\n",
        "all_files_in_dir = os.listdir(FULL_PATH_CSV + INCOMING_CSV_FOLDER)\n",
        "print(all_files_in_dir)\n",
        "\n",
        "# This searches all folders in the METADATA folder, not just incoming\n",
        "# But when I change it to incoming, it doesn't process CSVs at all\n",
        "all_csv_files_in_dir = [filename for filename in glob.iglob(FULL_PATH_CSV + INCOMING_CSV_FOLDER + '**.csv', recursive=False)]\n",
        "print(all_csv_files_in_dir)\n",
        "\n",
        "for csv_full_path in all_csv_files_in_dir:\n",
        "  csv_filename= csv_full_path.split(\"/\")[-1]\n",
        "  print(\"Processing \" + csv_filename + \"\\n\")\n",
        "  potentially_changed_hashid, error_hashid = processed_csv(csv_full_path, already_processed_hash_id)\n",
        "\n",
        "  try:\n",
        "    # Move file to processed_folder\n",
        "    shutil.move(csv_full_path,FULL_PATH_CSV + PROCESSED_CSV_FOLDER + csv_filename)\n",
        "\n",
        "    print(\"Moving \" + csv_filename + \" to processed folder\")\n",
        "\n",
        "    # Record log\n",
        "    full_path_filename_log = FULL_PATH_CSV + PROCESSED_LOGS_FOLDER + csv_filename[:-4] + \".txt\"\n",
        "    with open(full_path_filename_log , \"w+\") as log_file:\n",
        "        print(\"List of hashids skipped:\\n\", file=log_file)\n",
        "        print(*potentially_changed_hashid, sep=\"\\n\", file=log_file)\n",
        "\n",
        "        print(\"\\nList of hashids with errors:\\n\", file=log_file)\n",
        "        for t in potentially_changed_hashid:\n",
        "          line = ' '.join(str(x) for x in t)\n",
        "          print(line, sep=\"\\n\", file=log_file)\n",
        "\n",
        "\n",
        "    print(\"\\n Processing done \\n\")\n",
        "    \n",
        "  except Exception as e:\n",
        "          print(\"\\n\")\n",
        "          print(str(e))\n",
        "          print(\"Unable to process: \" + hashid)\n",
        "          error_hashid.append((hashid, str(e)))\n",
        "          continue\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "MASTER CSV ALREADY EXISTS\n",
            "['swingstates_footage_export.csv']\n",
            "['/content/drive/My Drive/Protect the Results Story Team/10 PTR DISTRIBUTION/Footage Library/METADATA/incoming_csv/swingstates_footage_export.csv']\n",
            "Processing swingstates_footage_export.csv\n",
            "\n",
            "Moving swingstates_footage_export.csv to processed folder\n",
            "\n",
            " Processing done \n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7KoFSo0arxTk"
      },
      "source": [
        "### Attribution\n",
        "\n",
        "This code was written by Jenisha Patel and very very slightly edited and reorganized by Brienne Hayes."
      ]
    }
  ]
}